version: "3.8"

services:
  namenode:
    image: uhopper/hadoop-namenode:3.3.4
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    networks:
      - hadoop

  datanode1:
    image: uhopper/hadoop-datanode:3.3.4
    container_name: datanode1
    depends_on:
      - namenode
    environment:
      - NAMENODE_URI=hdfs://namenode:9000
    ports:
      - "9864:9864"
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    networks:
      - hadoop

  datanode2:
    image: uhopper/hadoop-datanode:3.3.4
    container_name: datanode2
    depends_on:
      - namenode
    environment:
      - NAMENODE_URI=hdfs://namenode:9000
    ports:
      - "9865:9864"
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    networks:
      - hadoop

  resourcemanager:
    image: uhopper/hadoop-resourcemanager:3.3.4
    container_name: resourcemanager
    depends_on:
      - namenode
      - datanode1
      - datanode2
    ports:
      - "8088:8088"
    environment:
      - NAMENODE_URI=hdfs://namenode:9000
    networks:
      - hadoop

  nodemanager1:
    image: uhopper/hadoop-nodemanager:3.3.4
    container_name: nodemanager1
    depends_on:
      - resourcemanager
    environment:
      - RESOURCEMANAGER_URI=resourcemanager:8088
    networks:
      - hadoop

  nodemanager2:
    image: uhopper/hadoop-nodemanager:3.3.4
    container_name: nodemanager2
    depends_on:
      - resourcemanager
    environment:
      - RESOURCEMANAGER_URI=resourcemanager:8088
    networks:
      - hadoop

  spark-master:
    image: uhopper/spark-master:3.3.2
    container_name: spark-master
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      - SPARK_MODE=master
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    networks:
      - hadoop

  spark-worker1:
    image: uhopper/spark-worker:3.3.2
    container_name: spark-worker1
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    networks:
      - hadoop

  spark-worker2:
    image: uhopper/spark-worker:3.3.2
    container_name: spark-worker2
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    networks:
      - hadoop

  hive-metastore-db:
    image: postgres:13
    container_name: hive-metastore-db
    restart: always
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
    ports:
      - "5432:5432"
    networks:
      - hadoop

  hive-metastore:
    image: uhopper/hive-metastore:3.1.2
    container_name: hive-metastore
    depends_on:
      - hive-metastore-db
      - namenode
    environment:
      HIVE_METASTORE_DB_HOSTNAME: hive-metastore-db
      HIVE_METASTORE_DB_NAME: metastore
      HIVE_METASTORE_DB_USERNAME: hive
      HIVE_METASTORE_DB_PASSWORD: hive
    ports:
      - "9083:9083"
    networks:
      - hadoop

  hive-server:
    image: uhopper/hive-server:3.1.2
    container_name: hive-server
    depends_on:
      - hive-metastore
      - namenode
    environment:
      HIVE_METASTORE_URI: thrift://hive-metastore:9083
      HIVE_EXECUTION_ENGINE: spark
    ports:
      - "10000:10000"
    networks:
      - hadoop

volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:

networks:
  hadoop:
